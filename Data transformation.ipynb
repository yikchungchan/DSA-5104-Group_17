{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the outputs of *\"Execution Time for original data format.js\"*, we see that the running time for the daily quests vary from 17 seconds to 24 seconds. We need another data representation to speed up the execution time.\n",
    "\n",
    "After analyzing our query, we can see that our query can be divided into two parts: Finding \"asin\" for the required products in transaction_data, then look-up the product information in meta_data. Therefore, we are wondering if storing the information in transaction data would speed-up our query.\n",
    "\n",
    "In this file, I will do the tasks mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import required libraries:\n",
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "from datetime import timedelta, datetime\n",
    "from bson import ObjectId\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = {\n",
    "    \"review\": \"generated data/review_data.json\",\n",
    "    \"meta\": \"generated data/meta_data.json\",\n",
    "}\n",
    "\n",
    "# Load each JSON file using json.load() and convert directly to pandas DataFrame\n",
    "review_df = pd.read_json(file_paths[\"review\"], lines=True, encoding='utf-8')\n",
    "meta_df = pd.read_json(file_paths[\"meta\"], lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also read the sample data files:\n",
    "file_paths_sample = {\n",
    "    \"review\": \"generated data/review_data_sample.json\",\n",
    "    \"meta\": \"generated data/meta_data_sample.json\",\n",
    "    \"user\": \"generated data/user_data_sample.json\",\n",
    "    \"transaction\": \"generated data/transaction_data_sample.json\"\n",
    "}\n",
    "\n",
    "# Load each JSON file using json.load() and convert directly to pandas DataFrame\n",
    "review_sample_df = pd.read_json(file_paths_sample[\"review\"], lines=True, encoding='utf-8')\n",
    "meta_sample_df = pd.read_json(file_paths_sample[\"meta\"], lines=True, encoding='utf-8')\n",
    "user_sample_df = pd.read_json(file_paths_sample[\"user\"], lines=True, encoding='utf-8')\n",
    "transaction_sample_df = pd.read_json(file_paths_sample[\"transaction\"], lines=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6460965, 11)\n",
      "(1210967, 18)\n",
      "overall                                                           4\n",
      "vote                                                              1\n",
      "verified                                                      False\n",
      "reviewTime                                              08 14, 2000\n",
      "unixReviewTime                                            966211200\n",
      "reviewerID                                           A19646YDU8IH1I\n",
      "reviewerName                                    Robert Ian Farquhar\n",
      "asin                                                     B00000DMA8\n",
      "style                                     {'Edition:': ' Standard'}\n",
      "reviewText        Okay I admit it, the two main reasons I bought...\n",
      "summary                                                   Good Fun!\n",
      "Name: 945, dtype: object\n",
      "asin                                                      B00001XDVT\n",
      "title                                  Armorines: Project S.W.A.R.M.\n",
      "feature            [Great Condition, cleaned and tested, *Cartrid...\n",
      "description        [Alien bugs have swarmed to earth with a nasty...\n",
      "price                                                         $19.90\n",
      "imageURL           [https://images-na.ssl-images-amazon.com/image...\n",
      "imageURLHighRes    [https://images-na.ssl-images-amazon.com/image...\n",
      "also_buy           [B000031KJT, B00000F1GS, B00002SWA8, B00000K1X...\n",
      "also_view                                   [B00000J2W7, B000PBEQ2W]\n",
      "rank               [>#31,928 in Video Games (See Top 100 in Video...\n",
      "brand                                                Acclaim Studios\n",
      "category           [Video Games, Retro Gaming & Microconsoles, Ni...\n",
      "main_cat                                                 Video Games\n",
      "tech1                                                               \n",
      "tech2                                                               \n",
      "similar_item                                                        \n",
      "date                                                                \n",
      "fit                                                                 \n",
      "Name: 945, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# To check if the dataset is correctly loaded\n",
    "print(review_df.shape)\n",
    "print(meta_df.shape)\n",
    "print(review_df.iloc[945])\n",
    "print(meta_df.iloc[945])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge review_df and meta_df on 'asin' to obtain price for each review\n",
    "payment_methods = ['MasterCard', 'Visa', 'PayPal', 'Discover', 'Amex', 'Bitcoin']\n",
    "\n",
    "merged_df = review_df.merge(meta_df[['asin', 'price']], on='asin', how='left')\n",
    "valid_prices = merged_df['price'].str.startswith('$', na=False)\n",
    "merged_df = merged_df[valid_prices]\n",
    "merged_df['price'] = merged_df['price'].str[1:].astype(float)\n",
    "\n",
    "merged_df['copy'] = [random.randint(1, 10) for _ in range(len(merged_df))]\n",
    "merged_df['totalPrice'] = merged_df['price'] * merged_df['copy']\n",
    "\n",
    "seconds_in_a_day = 86400\n",
    "random_seconds = [random.randint(0, 7 * seconds_in_a_day) for _ in range(len(merged_df))]\n",
    "review_times = pd.to_datetime(merged_df['unixReviewTime'], unit='s')\n",
    "transaction_times = review_times - pd.to_timedelta(random_seconds, unit='s')\n",
    "merged_df['transactionTime'] = transaction_times\n",
    "\n",
    "\n",
    "merged_df['paymentMethod'] = [random.choice(payment_methods) for _ in range(len(merged_df))]\n",
    "merged_df['transactionID'] = [str(ObjectId()) for _ in range(len(merged_df))]\n",
    "\n",
    "# Adding \"Overall\" Column\n",
    "transaction_df = merged_df[['transactionID', 'transactionTime', 'asin','reviewerID', 'copy', 'totalPrice', 'paymentMethod', 'overall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              transactionID  transactionTime      reviewerID  copy  \\\n",
      "0  65463da2e6e35c4ffca6d932    1098525317000  A36Y3ZMIWOV2DA     7   \n",
      "1  65463da2e6e35c4ffca6d933    1098679454000  A35R5A5PN4SZUL     7   \n",
      "2  65463da2e6e35c4ffca6d934    1098298562000  A2PXOP8AOGXQCO     7   \n",
      "3  65463da2e6e35c4ffca6d935    1098414197000  A1WX4CIGZ6ZHNG     3   \n",
      "4  65463da2e6e35c4ffca6d936    1519082412000  A3HWCOEZBA8Z23     9   \n",
      "\n",
      "   totalPrice paymentMethod                                       product_info  \n",
      "0      328.23        PayPal  {'asin': 'B00004SQPD', 'title': 'PlayStation 2...  \n",
      "1      328.23      Discover  {'asin': 'B00004SQPD', 'title': 'PlayStation 2...  \n",
      "2      328.23          Visa  {'asin': 'B00004SQPD', 'title': 'PlayStation 2...  \n",
      "3      140.67    MasterCard  {'asin': 'B00004SQPD', 'title': 'PlayStation 2...  \n",
      "4      422.01          Amex  {'asin': 'B00004SQPD', 'title': 'PlayStation 2...  \n"
     ]
    }
   ],
   "source": [
    "# Transform sample transaction data\n",
    "merged_df = pd.merge(transaction_sample_df, meta_sample_df[['asin', 'title', 'price', 'also_buy', 'also_view']], on='asin', how='left')\n",
    "\n",
    "# We'll create a new column 'product_info' which will be a dictionary of the required fields\n",
    "merged_df['product_info'] = merged_df.apply(lambda row: {\n",
    "    'asin': row['asin'],\n",
    "    'title': row['title'],\n",
    "    'price': row['price'],\n",
    "    'also_buy': row['also_buy'],\n",
    "    'also_view': row['also_view']\n",
    "}, axis=1)\n",
    "\n",
    "merged_df.drop(columns=['asin','title', 'description', 'price', 'also_buy', 'also_view'], inplace=True)\n",
    "\n",
    "print(merged_df.head())  # Check the results\n",
    "\n",
    "output_dir = \"G:\\\\DSA5104 Project\\\\generated data\"\n",
    "merged_df.to_json(os.path.join(output_dir, 'transaction_data_sample_t.json'), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MongoDB, we also received a warning: Some of the imported documents contained unbounded arrays that may degrade efficiency.\n",
    "\n",
    "To deal with this problem, I use these two methods:\n",
    "1. do not include descriptions in 'product_info' fields\n",
    "2. limit the length of 'also_view' and 'also_buy' to 15 elements (elements exceed 15 will be ignored)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              transactionID     transactionTime      reviewerID  copy  \\\n",
      "0  654e19d0e6e35c3d6c3cb471 2015-10-13 05:24:01  A1HP7NVNPFMA4N     3   \n",
      "1  654e19d0e6e35c3d6c3cb472 2015-07-26 07:21:03  A1JGAP0185YJI6     6   \n",
      "2  654e19d0e6e35c3d6c3cb473 2015-02-19 16:02:56  A1YJWEXHQBWK2B     2   \n",
      "3  654e19d0e6e35c3d6c3cb474 2015-02-14 21:16:41  A2204E1TH211HT     1   \n",
      "4  654e19d0e6e35c3d6c3cb475 2014-12-23 23:10:52  A2RF5B5H74JLPE     1   \n",
      "\n",
      "   totalPrice paymentMethod  overall  \\\n",
      "0       23.85          Visa        5   \n",
      "1       47.70          Visa        4   \n",
      "2       15.90          Visa        3   \n",
      "3        7.95       Bitcoin        2   \n",
      "4        7.95          Visa        5   \n",
      "\n",
      "                                        product_info  \n",
      "0  {'asin': '0700026657', 'title': 'Anno 2070', '...  \n",
      "1  {'asin': '0700026657', 'title': 'Anno 2070', '...  \n",
      "2  {'asin': '0700026657', 'title': 'Anno 2070', '...  \n",
      "3  {'asin': '0700026657', 'title': 'Anno 2070', '...  \n",
      "4  {'asin': '0700026657', 'title': 'Anno 2070', '...  \n"
     ]
    }
   ],
   "source": [
    "# Then, we apply the codes above to the whole dataset:\n",
    "merged_df = pd.merge(transaction_df, meta_df[['asin', 'title', 'price', 'also_buy', 'also_view']], on='asin', how='left')\n",
    "\n",
    "# Limit the 'also_buy' and 'also_view' arrays to the first 15 elements\n",
    "def limit_array_length(row, field_name):\n",
    "    return row[field_name][:15] if isinstance(row[field_name], list) else row[field_name]\n",
    "\n",
    "merged_df['product_info'] = merged_df.apply(lambda row: {\n",
    "    'asin': row['asin'],\n",
    "    'title': row['title'],\n",
    "    'price': row['price'],\n",
    "    'also_buy': limit_array_length(row, 'also_buy'),\n",
    "    'also_view': limit_array_length(row, 'also_view')\n",
    "}, axis=1)\n",
    "\n",
    "merged_df.drop(columns=['asin', 'title', 'price', 'also_buy', 'also_view'], inplace=True)\n",
    "print(merged_df.head())  # Check the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6453130, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"G:\\\\DSA5104 Project\\\\generated data\\\\transaction_data_t.json\"\n",
    "chunk_size = 1000  \n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    for i in range(0, len(merged_df), chunk_size):\n",
    "        chunk = merged_df.iloc[i:i + chunk_size]\n",
    "        chunk_json = chunk.to_json(orient='records', lines=True)\n",
    "        file.write(chunk_json)\n",
    "        file.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
